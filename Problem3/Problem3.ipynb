{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Result:     e  o  ea e  a ea e   ea  ea ea e  a  ea e     oo   e ea e  e  e   a  ea e  ea e  a       o e    ea  ea ea      e  ea ea e   e  a e  e  e e     ea e  ea ea ea    ea e       o    ea ea ea   e  e  ea ea e    ea ea ea ea ea ea e  a ea e     oo          o ea ea e  a  a ea  e  o  e    ea e e  ea   ea e    o  o  a  e e  a ea e \n",
      "Loss value: \n",
      " 2.759727954864502\n",
      "Output Result:  iere  nl thsy hareanion  af Larsage  mf sorem Ihsumeare lasle ue toe massrea  hare tom  re  hneed de n tn tom  norem bl hone e d ho uaree den s d ahres shedh bon e toru   en aledhi l hade nesladaa sor mre moing torsueda lassage of sorem Ihsumeau re   io su tore there  nn t tny hen  tnearyansan  hedde  in the maddle hf sadeh\n",
      "Loss value: \n",
      " 1.8897075653076172\n",
      "Output Result:  here  se gany pareadion  af Lassage  af Lorem Ipsumyavailablabut ihe madsreth hare sumfereg alieraieon tn tome forem by inge ted humeurragdonised hords Ihidh don't look eren slightly pedeevabledIf you mre going to use a passage of sorem Ipsumyau ne   to be sure there  sndt onytheng t  arrassang hedden in the maddle of sedth\n",
      "Loss value: \n",
      " 1.2425732612609863\n",
      "Output Result:  here are many variation  of passage  of Lorem Ipsumyavailablebut the madority have suffered alteration en some form, by injerted humourrandomised words whidh don't look even slightly bedievablebIf you are going to use a passage of Lorem Ipsumyou need to be sure there isn't anything emdarrasseng hidden in the middle of tedtl\n",
      "Loss value: \n",
      " 0.8527944087982178\n",
      "Output Result:  here are many variation  of passages of Lorem Ipsumyavailablebut the majority have suffered alteration in some form, by injected humourrandomised words which don't look even slightly believable.If you are going to use a passage of Lorem Ipsumyou need to be sure there isn't anything embarrassing hidden in the middle of tedtd\n",
      "Loss value: \n",
      " 0.6463523507118225\n",
      "Output Result:  here are many variations of passages of Lorem Ipsumyavailablebut the majority have suffered alteration in some form, by injected humourrandomised words which don't look even slightly believable.If you are going to use a passage of Lorem Ipsumyou need to be sure there isn't anything embarrassing hidden in the middle of tedt.\n",
      "Loss value: \n",
      " 0.5347559452056885\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[주의!]\n",
    "상단 코드는 수정하지 마세요\n",
    "\"\"\"\n",
    "###################################################################\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "############# 해당 부분을 구현하시면 됩니다#########################\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super().__init__()        \n",
    "        \n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###################### 구현 부분 끝  ##############################\n",
    "###################################################################\n",
    "\"\"\"\n",
    "[주의!]\n",
    "하단 코드는 수정하지 마세요\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##SEED값 고정\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    " \n",
    "\n",
    "\n",
    "sentence = (\"There are many variations of passages of Lorem Ipsum available\"\n",
    "            \"but the majority have suffered alteration in some form, by injected humour\"\n",
    "            \"randomised words which don't look even slightly believable.\"\n",
    "            \"If you are going to use a passage of Lorem Ipsum\"\n",
    "            \"you need to be sure there isn't anything embarrassing hidden in the middle of text.\")\n",
    "\n",
    "# make dictionary\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "sequence_length = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# data setting\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "\n",
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot).to(DEVICE)\n",
    "Y = torch.LongTensor(y_data).to(DEVICE)\n",
    "\n",
    "model = MyModel(dic_size, hidden_size, 2).to(DEVICE)\n",
    "\n",
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "# start training\n",
    "for i in range(1, 601):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X).to(DEVICE)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "    if i % 100 == 0:\n",
    "        print(\"Output Result: \", predict_str)\n",
    "        print(\"Loss value: \\n\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lee",
   "language": "python",
   "name": "lee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
